{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "## Suggestion Mining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  670k  100  670k    0     0  5054k      0 --:--:-- --:--:-- --:--:-- 5076k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  168k  100  168k    0     0  1869k      0 --:--:-- --:--:-- --:--:-- 1868k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  208k  100  208k    0     0  2894k      0 --:--:-- --:--:-- --:--:-- 2930k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For test_unseen dataset\n",
    "test1_df = pd.read_csv('test_unseen.csv', \n",
    "                 names=['id','text'], header=0)\n",
    "\n",
    "test1_texts, test1_id = test1_df[\"text\"].to_list(), test1_df[\"id\"].to_list() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> Data Pre-Processing Techniques:\n",
    "\n",
    "Preprocessing data simply means putting it in a predictable and analyzable form for achieving the goal of the task. A task in this context combines an approach and a domain. An example of a Task is to extract the top keywords using the TF-IDF (approach) from Tweets (domain).  \n",
    "\n",
    "Data Pre-processing Techniques:  \n",
    "\n",
    "Stop word Removal: A language's stop words are a group of frequently used terms. Stop words in English include \"a,\" \"the,\" \"is,\" \"are,\" and others. Stop words are used with the theory that by eliminating low information terms from text, we may concentrate on the key words instead.  \n",
    "\n",
    "Lemmatization: The purpose of lemmatization, like that of stemming, is to eliminate inflections and map a word to its root form. The sole distinction is that lemmatization attempts to carry out the process correctly. It truly changes words to their true roots, not merely cuts things off. Better, for instance, would translate to \"good.\" For mappings, it might make use of a dictionary like WordNet.  \n",
    "\n",
    "Tokenization: Tokenization refers to the process of breaking up strings into single words without any spaces or tabs. We will also change the case of each word in the string to lower case in this stage. We can simply perform these thanks to the NLTK tokenize module.  \n",
    "\n",
    "Punctuation Removal: The removal of punctuation marks is a crucial NLP preprocessing step since these markings, which are used to separate text into sentences, paragraphs, and phrases, have an impact on the outcomes of any text processing strategy, particularly those that depend on word and phrase occurrence rates.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/basabdattachaudhury/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/basabdattachaudhury/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/basabdattachaudhury/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/basabdattachaudhury/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/basabdattachaudhury/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Here as a pre-processing technique we are removing the punctuations, tokenizing the text , removing the stopwords and lemmatizing the text\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Preprocess_listofSentence(train_tex):\n",
    "    preprocess_list = []\n",
    "    for sentence in train_tex :\n",
    "        sentence_w_punct = \"\".join([i.lower() for i in sentence if i not in string.punctuation])\n",
    "\n",
    "        sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())\n",
    "\n",
    "        tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)\n",
    "\n",
    "        words_w_stopwords = [i for i in tokenize_sentence if i not in stopwords]      \n",
    "\n",
    "        words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)         \n",
    "\n",
    "        sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in words or not w.isalpha())\n",
    "\n",
    "        preprocess_list.append(sentence_clean)\n",
    "\n",
    "    return preprocess_list\n",
    "\n",
    "#For training dataset\n",
    "preprocess_list = Preprocess_listofSentence(train_df['text'])\n",
    "df = pd.DataFrame({'col': preprocess_list})\n",
    "\n",
    "train_df['text'] = df['col']\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "\n",
    "assert len(train_texts) == len(train_labels) == 5440\n",
    "\n",
    "\n",
    "#For test dataset\n",
    "preprocess_list2 = Preprocess_listofSentence(test_df['text'])\n",
    "df2 = pd.DataFrame({'col': preprocess_list2})\n",
    "\n",
    "test_df['text'] = df2['col']\n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list()\n",
    "\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "\n",
    "#For test_unseen dataset\n",
    "preprocess_list3 = Preprocess_listofSentence(test1_df['text'])\n",
    "df3 = pd.DataFrame({'col': preprocess_list3})\n",
    "\n",
    "test1_df['text'] = df3['col']\n",
    "test1_texts, test1_labels = test1_df[\"text\"].to_list(), test1_df[\"id\"].to_list() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/basabdattachaudhury/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7676470588235295"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "\n",
    "Count_Vect = CountVectorizer()\n",
    "X_train_tf = Count_Vect.fit_transform(train_texts)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_tf)\n",
    "\n",
    "df=pd.DataFrame(X_train_tfidf.toarray(),columns=Count_Vect.get_feature_names())\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_labels)\n",
    "\n",
    "X_test_tf = Count_Vect.transform(test_texts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_tf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = []    # save your predictions on the test set into this list\n",
    "\n",
    "\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "predictions = predicted.tolist()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "\n",
    "> Why not Accuracy?\n",
    "The number of right predictions out of all predictions produced is typically how we measure a model's accuracy when we develop one for a classification task. However, classification accuracy alone is frequently insufficient to make this choice.Accuracy may be deceptive. When a model has more predictive power for a given problem, it may be preferable to choose it over one with higher accuracy.For instance, a model can predict the value of the majority class for all predictions in a problem with a significant class imbalance and obtain high classification accuracy; the issue is that this model is not applicable to the problem domain. This is known as Accuracy Paradox. To evaluate a classifier for challenges like these, extra measurements are needed.  \n",
    "An evaluation metric which will work better than accuracy for a classification task such as suggestion detection is F1 score. It combines both the evaluation metric precision and recall.\n",
    "The F1 Score is the 2*((precision*recall)/(precision+recall)). \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1366120218579235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "    '''\n",
    "  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  # check that labels and predictions are of same length\n",
    "    assert len(labels) == len(predictions)\n",
    "    score = 0.0\n",
    "   \n",
    "    TruePos=0\n",
    "    FalsePos=0\n",
    "    FalseNeg = 0\n",
    "    for label, prediction in zip(labels, predictions):\n",
    "        if label == prediction:\n",
    "            if label == 1:\n",
    "                TruePos += 1\n",
    "        else:\n",
    "            if label == 1:\n",
    "                FalseNeg += 1\n",
    "            else:\n",
    "                FalsePos += 1\n",
    "    precision = TruePos/(TruePos+FalsePos)\n",
    "    recall = TruePos/(TruePos+FalseNeg)\n",
    "    score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "\n",
    "    return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676470588235295"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_labels,predicted, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "> CountVectorizer using N-gram:  \n",
    "The scikit-learn package CountVectorizer employs count vectorization to turn a group of text documents into a matrix of token counts. To assist you with identifying recurring text patterns in a corpus of text documents, such as web pages or product descriptions, CountVectorizer can produce a matrix listing the frequency of each word or phrase.\n",
    "Below, we'll use CountVectorizer to carry out some fundamental n-gram analysis. N-grams, also known as Q-grams or shingles, are single- or multi-word phrases that can be found in texts. They can help data scientists comprehend the issue by revealing its underlying structure, or they can be utilized in NLP models for text classification. We can adjust the ngram_range argument. If we set the CountVectorizer to 1, 1 to return unigrams or single words. Increasing the ngram_range will mean the vocabulary is expanded from single words to short phrases of your desired lengths. For example, setting the ngram_range to 2, 2 will return bigrams (2-grams) or two-word phrases. This might improve the performance of our suggestion detector. We can also select n-grams of multiple sizes all at once by setting an unequal ngram_range. For example, setting the range from 1, 5 will return n-grams containing one, two, three, four, and five words after stop words have been removed. But, this adds some potentially useful phrases to the vocabulary, as well as some nonsense. N-grams remove background noise from the data you're analyzing. Since search term data is the foundation of the search industry, n-grams are crucial for reducing the noise created by thousands of rows of data on individual search terms.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfkzM3DRce14"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  670k  100  670k    0     0  5574k      0 --:--:-- --:--:-- --:--:-- 5583k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  168k  100  168k    0     0  2376k      0 --:--:-- --:--:-- --:--:-- 2402k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  208k  100  208k    0     0  3081k      0 --:--:-- --:--:-- --:--:-- 3104k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: nltk\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.cimport nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440\n",
    "\n",
    "#Pre-processing the datasets\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Preprocess_listofSentence(train_tex):\n",
    "    preprocess_list = []\n",
    "    for sentence in train_tex :\n",
    "        sentence_w_punct = \"\".join([i.lower() for i in sentence if i not in string.punctuation])\n",
    "\n",
    "        sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())\n",
    "\n",
    "        tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)\n",
    "\n",
    "        words_w_stopwords = [i for i in tokenize_sentence if i not in stopwords]      \n",
    "\n",
    "        words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)         \n",
    "\n",
    "        sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in words or not w.isalpha())\n",
    "\n",
    "        preprocess_list.append(sentence_clean)\n",
    "\n",
    "    return preprocess_list\n",
    "\n",
    "preprocess_list = Preprocess_listofSentence(train_df['text'])\n",
    "df = pd.DataFrame({'col': preprocess_list})\n",
    "\n",
    "train_df['text'] = df['col']\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "\n",
    "assert len(train_texts) == len(train_labels) == 5440\n",
    "\n",
    "\n",
    "preprocess_list2 = Preprocess_listofSentence(test_df['text'])\n",
    "df2 = pd.DataFrame({'col': preprocess_list2})\n",
    "\n",
    "test_df['text'] = df2['col']\n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list()\n",
    "\n",
    "assert len(test_texts) == len(test_labels) == 1360\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "#used reference from NLP lab classes\n",
    "\n",
    "\n",
    "cn_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "Xtrain_cn = cn_vect.fit_transform(train_texts)\n",
    "tf_transformer = TfidfTransformer()\n",
    "train_tfidf = tf_transformer.fit_transform(Xtrain_cn)\n",
    "train_aspects = Xtrain_cn.toarray() + train_tfidf.toarray()\n",
    "\n",
    "#On Test Data Set\n",
    "Xtest_cn = cn_vect.transform(test_texts) \n",
    "test_tfidf = tf_transformer.transform(Xtest_cn)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_tfidf.toarray(), train_labels)\n",
    "\n",
    "\n",
    "# Evaluate on the test set.\n",
    "\n",
    "Xtest_cn = cn_vect.transform(test_texts)\n",
    "test_tfidf = tf_transformer.transform(Xtest_cn)\n",
    "predictions = clf.predict(test_tfidf.toarray())\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  670k  100  670k    0     0  5754k      0 --:--:-- --:--:-- --:--:-- 5776k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  168k  100  168k    0     0  2333k      0 --:--:-- --:--:-- --:--:-- 2368k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  208k  100  208k    0     0  2767k      0 --:--:-- --:--:-- --:--:--     0 --:--:-- 2773k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: nltk\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.cimport nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test1_df = pd.read_csv('test_unseen.csv', \n",
    "                 names=['id', 'text'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test1_texts, test1_id = test_df[\"text\"].to_list(), test_df[\"id\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(train_texts) == len(train_labels) == 5440\n",
    "\n",
    "\n",
    "#Pre-processing the training and test_unseen data\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Preprocess_listofSentence(train_tex):\n",
    "    preprocess_list = []\n",
    "    for sentence in train_tex :\n",
    "        sentence_w_punct = \"\".join([i.lower() for i in sentence if i not in string.punctuation])\n",
    "\n",
    "        sentence_w_num = ''.join(i for i in sentence_w_punct if not i.isdigit())\n",
    "\n",
    "        tokenize_sentence = nltk.tokenize.word_tokenize(sentence_w_num)\n",
    "\n",
    "        words_w_stopwords = [i for i in tokenize_sentence if i not in stopwords]      \n",
    "\n",
    "        words_lemmatize = (lemmatizer.lemmatize(w) for w in words_w_stopwords)         \n",
    "\n",
    "        sentence_clean = ' '.join(w for w in words_lemmatize if w.lower() in words or not w.isalpha())\n",
    "\n",
    "        preprocess_list.append(sentence_clean)\n",
    "\n",
    "    return preprocess_list\n",
    "\n",
    "preprocess_list = Preprocess_listofSentence(train_df['text'])\n",
    "df = pd.DataFrame({'col': preprocess_list})\n",
    "\n",
    "train_df['text'] = df['col']\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "\n",
    "assert len(train_texts) == len(train_labels) == 5440\n",
    "\n",
    "\n",
    "preprocess_list3 = Preprocess_listofSentence(test1_df['text'])\n",
    "df3 = pd.DataFrame({'col': preprocess_list3})\n",
    "\n",
    "test1_df['text'] = df3['col']\n",
    "test1_texts, test1_id = test1_df[\"text\"].to_list(), test1_df[\"id\"].to_list() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#using our previously created feature extraction method and classifier on the test_unseen data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, accuracy_score\n",
    "\n",
    "#used reference from NLP lab classes\n",
    "\n",
    "cn_vect = CountVectorizer(ngram_range=(2, 2))\n",
    "Xtrain_cn = cn_vect.fit_transform(train_texts)\n",
    "tf_transformer = TfidfTransformer()\n",
    "train_tfidf = tf_transformer.fit_transform(Xtrain_cn)\n",
    "\n",
    "train_aspects = Xtrain_cn.toarray() + train_tfidf.toarray()\n",
    "\n",
    "test1_counts = cn_vect.transform(test1_texts) \n",
    "test_tfidf = tf_transformer.transform(test1_counts)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_tfidf.toarray(), train_labels)\n",
    "\n",
    "\n",
    "# Now let's do some predictions on the test set.\n",
    "test1_counts = cn_vect.transform(test1_texts)\n",
    "test_tfidf = tf_transformer.transform(test1_counts)\n",
    "predictions = clf.predict(test_tfidf.toarray())\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [],
   "source": [
    "# Preparing submission for Kaggle\n",
    "StudentID = \"22225649_Chaudhury\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
    "       \"Expected\": predictions}\n",
    "\n",
    "sub_df = pd.DataFrame(sub)\n",
    "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
    "# Please upload the file as a submission on the competition page\n",
    "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
    "sub_df.to_csv(f\"22225649.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "> We are using n-gram approach for the feature extraction on the given test_unseen dataset.\n",
    "First we pre-processed the test_unseen dataset and then applied the n-gram with ngram_range=(2, 2) i.e. a bigram approach on this. Then we train with the given training data  with Multinomial Naïve Bayes Classifier and then predict the labels for the test_unseen data. We get a F-1 score of 0.7847 which is above the baseline score.\n",
    "In text mining and activities involving natural language processing, n-grams of texts are frequently used. They are essentially a collection of words that frequently occur in a certain window, and while computing the n-grams, you usually advance one word (although you can move X words forward in more advanced scenarios). Popular in Natural Language Processing is the Bayesian learning method known as the Multinomial Naive Bayes algorithm (NLP). Using the Bayes principle, the computer makes an educated prediction about the tag of a text, such as an email or news article. It determines the likelihood of each tag for a particular sample and outputs the tag with the highest likelihood.\n",
    "My F-1 score is 0.7847.\n",
    "Yes, it was better than the selected baseline. It assists in producing effective word embeddings for unidentified words, enhancing classification performance in tasks based on informal texts, where an abundance of unidentified words present, such as in suggestions.\n",
    "\n",
    "References: https://www.upgrad.com/blog/multinomial-naive-bayes-explained/\n",
    "https://kavita-ganesan.com/what-are-n-grams/\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
